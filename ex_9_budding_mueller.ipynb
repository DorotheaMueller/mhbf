{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 Reinforcement Learning I \n",
    "## Céline Budding && Dorothea Müller "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Good vs. bad Döner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_states = 50\n",
    "# good_doener_val = 1000\n",
    "# bad_doener_val = 100\n",
    "\n",
    "# rewards = np.repeat(-10, n_states)\n",
    "# rewards[0] = bad_doener_val\n",
    "# rewards[-1] = good_doener_val\n",
    "# rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  100,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,\n",
       "        -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,\n",
       "        -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,\n",
       "        -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,  -10,\n",
       "        -10,  -10,  -10,  -10,  -10,  -10, 1000,    0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_states = 50\n",
    "good_doener_val = 1000\n",
    "bad_doener_val = 100\n",
    "\n",
    "rewards = np.repeat(-10, n_states + 2)\n",
    "rewards[0] = 0\n",
    "rewards[1] = bad_doener_val\n",
    "rewards[-2] = good_doener_val\n",
    "rewards[-1] = 0\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        ],\n",
       "       [0.81174589, 0.59129522],\n",
       "       [0.04350683, 0.52963806],\n",
       "       [0.17375667, 0.02515537],\n",
       "       [0.16063962, 0.14156799],\n",
       "       [0.23326049, 0.92623894],\n",
       "       [0.93776337, 0.20582071],\n",
       "       [0.77513465, 0.31741799],\n",
       "       [0.94468473, 0.99980799],\n",
       "       [0.99692812, 0.54539272],\n",
       "       [0.42755092, 0.98826392],\n",
       "       [0.75702191, 0.87753333],\n",
       "       [0.10605975, 0.66241641],\n",
       "       [0.24368393, 0.75640703],\n",
       "       [0.14066633, 0.9129011 ],\n",
       "       [0.3058302 , 0.07797872],\n",
       "       [0.47837788, 0.81808745],\n",
       "       [0.90573714, 0.92672477],\n",
       "       [0.82562413, 0.24842556],\n",
       "       [0.62655412, 0.08977717],\n",
       "       [0.51385853, 0.1298469 ],\n",
       "       [0.92518781, 0.99930269],\n",
       "       [0.03206189, 0.81806609],\n",
       "       [0.50751702, 0.59819042],\n",
       "       [0.02597505, 0.01007898],\n",
       "       [0.85443797, 0.50188314],\n",
       "       [0.54780293, 0.27801726],\n",
       "       [0.66937598, 0.31065273],\n",
       "       [0.79966362, 0.77799508],\n",
       "       [0.3071286 , 0.63034948],\n",
       "       [0.46810939, 0.17909965],\n",
       "       [0.15263979, 0.25922443],\n",
       "       [0.8472151 , 0.75381485],\n",
       "       [0.52702215, 0.2043471 ],\n",
       "       [0.76392494, 0.61797801],\n",
       "       [0.17622658, 0.28834037],\n",
       "       [0.11481594, 0.40742846],\n",
       "       [0.46613782, 0.73030098],\n",
       "       [0.12031397, 0.43898792],\n",
       "       [0.47658931, 0.51659856],\n",
       "       [0.38474073, 0.02007559],\n",
       "       [0.54828666, 0.55195221],\n",
       "       [0.83966678, 0.29911827],\n",
       "       [0.28772453, 0.08018131],\n",
       "       [0.92047237, 0.34235112],\n",
       "       [0.71719093, 0.01870787],\n",
       "       [0.79687898, 0.21880807],\n",
       "       [0.20633754, 0.73799256],\n",
       "       [0.04036048, 0.36394692],\n",
       "       [0.51046732, 0.07555606],\n",
       "       [0.46251549, 0.9450628 ],\n",
       "       [0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each state, two actions are possible.\n",
    "# First q values note down-movement.\n",
    "q_val = np.random.random((n_states + 2, 2))\n",
    "q_val[0] = np.array([0, 0])\n",
    "q_val[-1] = np.array([0, 0])\n",
    "\n",
    "# TODO: worry about the termination (end state)\n",
    "q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q, state, eps=0.1):\n",
    "    \"\"\"Return -1 for down, +1 for up.\"\"\"\n",
    "    action = None\n",
    "    # Check if terminated. TODO: how to do termination\n",
    "#     if state == 0 or state == len(q):\n",
    "#         return action\n",
    "    if False:\n",
    "        pass\n",
    "   \n",
    "    elif np.random.random() < 1-eps:\n",
    "        if q[state][0] > q[state][1]:\n",
    "            action = -1\n",
    "        else:\n",
    "            action = 1\n",
    "    # Epsilon.\n",
    "    else:\n",
    "        action = np.random.choice([1, -1])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA algorithm.\n",
    "episodes = 1\n",
    "\n",
    "for _ in range(episodes):\n",
    "    # Random starting state.\n",
    "    state = np.random.randint(n_states)\n",
    "    # Choose action using eps policy derived from q.\n",
    "    action = choose_action(q_val, state)\n",
    "    reward = rewards[state + action]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SARSA algorithm.\n",
    "# episodes = 10\n",
    "# eta = 0.1\n",
    "# gamma = 1\n",
    "\n",
    "# for _ in range(episodes):\n",
    "#     # Random starting state.\n",
    "#     state = np.random.randint(n_states)\n",
    "#     # Choose action using policy derived from Q.\n",
    "#     action = choose_action(q, state)\n",
    "#     while action is not None:\n",
    "#         state = state + action\n",
    "#         # Get reward of action.\n",
    "#         reward = rewards[state]\n",
    "#         new_action = choose_action(q, state)\n",
    "#         if new_action is None:\n",
    "#             break\n",
    "#         new_state = state + new_action\n",
    "#         ## TODO: how does this update work?\n",
    "#         print(q)\n",
    "#         q = q + eta * (reward + gamma*q[new_state] - q)\n",
    "#         break\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
